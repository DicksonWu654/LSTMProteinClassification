{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hi! This is my code for protein classification. \n### Given a sequence of amino acids (there's about 20 unique amino acids) the Bi-Directional LSTM has to predict the function (20 different ones) of that protein!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import stuff for dataprocessing\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the data. seq has all the sequences of the DNA while the no_dups has the function\nno_dups = pd.read_csv(\"../input/protein-data-set/pdb_data_no_dups.csv\")\nseq = pd.read_csv(\"../input/protein-data-set/pdb_data_seq.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#lookin at what's inside\nno_dups.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"  structureId         classification experimentalTechnique macromoleculeType  \\\n0        100D         DNA-RNA HYBRID     X-RAY DIFFRACTION    DNA/RNA Hybrid   \n1        101D                    DNA     X-RAY DIFFRACTION               DNA   \n2        101M       OXYGEN TRANSPORT     X-RAY DIFFRACTION           Protein   \n3        102D                    DNA     X-RAY DIFFRACTION               DNA   \n4        102L  HYDROLASE(O-GLYCOSYL)     X-RAY DIFFRACTION           Protein   \n\n   residueCount  resolution  structureMolecularWeight  \\\n0            20        1.90                   6360.30   \n1            24        2.25                   7939.35   \n2           154        2.07                  18112.80   \n3            24        2.20                   7637.17   \n4           165        1.74                  18926.61   \n\n           crystallizationMethod  crystallizationTempK  densityMatthews  \\\n0  VAPOR DIFFUSION, HANGING DROP                   NaN             1.78   \n1                            NaN                   NaN             2.00   \n2                            NaN                   NaN             3.09   \n3  VAPOR DIFFUSION, SITTING DROP                 277.0             2.28   \n4                            NaN                   NaN             2.75   \n\n   densityPercentSol                                        pdbxDetails  \\\n0              30.89             pH 7.00, VAPOR DIFFUSION, HANGING DROP   \n1              38.45                                                NaN   \n2              60.20  3.0 M AMMONIUM SULFATE, 20 MM TRIS, 1MM EDTA, ...   \n3              46.06  pH 7.00, VAPOR DIFFUSION, SITTING DROP, temper...   \n4              55.28                                                NaN   \n\n   phValue  publicationYear  \n0      7.0           1994.0  \n1      NaN           1995.0  \n2      9.0           1999.0  \n3      7.0           1995.0  \n4      NaN           1993.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>structureId</th>\n      <th>classification</th>\n      <th>experimentalTechnique</th>\n      <th>macromoleculeType</th>\n      <th>residueCount</th>\n      <th>resolution</th>\n      <th>structureMolecularWeight</th>\n      <th>crystallizationMethod</th>\n      <th>crystallizationTempK</th>\n      <th>densityMatthews</th>\n      <th>densityPercentSol</th>\n      <th>pdbxDetails</th>\n      <th>phValue</th>\n      <th>publicationYear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100D</td>\n      <td>DNA-RNA HYBRID</td>\n      <td>X-RAY DIFFRACTION</td>\n      <td>DNA/RNA Hybrid</td>\n      <td>20</td>\n      <td>1.90</td>\n      <td>6360.30</td>\n      <td>VAPOR DIFFUSION, HANGING DROP</td>\n      <td>NaN</td>\n      <td>1.78</td>\n      <td>30.89</td>\n      <td>pH 7.00, VAPOR DIFFUSION, HANGING DROP</td>\n      <td>7.0</td>\n      <td>1994.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101D</td>\n      <td>DNA</td>\n      <td>X-RAY DIFFRACTION</td>\n      <td>DNA</td>\n      <td>24</td>\n      <td>2.25</td>\n      <td>7939.35</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.00</td>\n      <td>38.45</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1995.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101M</td>\n      <td>OXYGEN TRANSPORT</td>\n      <td>X-RAY DIFFRACTION</td>\n      <td>Protein</td>\n      <td>154</td>\n      <td>2.07</td>\n      <td>18112.80</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.09</td>\n      <td>60.20</td>\n      <td>3.0 M AMMONIUM SULFATE, 20 MM TRIS, 1MM EDTA, ...</td>\n      <td>9.0</td>\n      <td>1999.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102D</td>\n      <td>DNA</td>\n      <td>X-RAY DIFFRACTION</td>\n      <td>DNA</td>\n      <td>24</td>\n      <td>2.20</td>\n      <td>7637.17</td>\n      <td>VAPOR DIFFUSION, SITTING DROP</td>\n      <td>277.0</td>\n      <td>2.28</td>\n      <td>46.06</td>\n      <td>pH 7.00, VAPOR DIFFUSION, SITTING DROP, temper...</td>\n      <td>7.0</td>\n      <td>1995.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>102L</td>\n      <td>HYDROLASE(O-GLYCOSYL)</td>\n      <td>X-RAY DIFFRACTION</td>\n      <td>Protein</td>\n      <td>165</td>\n      <td>1.74</td>\n      <td>18926.61</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.75</td>\n      <td>55.28</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1993.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looks like they have DNA, DNA/RNA Hybrids and Proteins... So I'll filter for just protein because that should simplify things... Plus I'm doing protein folding after all\nno_dups_proteins = no_dups[no_dups.macromoleculeType == \"Protein\"]\nseq_proteins = seq[seq.macromoleculeType == \"Protein\"]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make sure everything only proteins\nno_dups_proteins.head(), seq_proteins.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(  structureId         classification experimentalTechnique macromoleculeType  \\\n 2        101M       OXYGEN TRANSPORT     X-RAY DIFFRACTION           Protein   \n 4        102L  HYDROLASE(O-GLYCOSYL)     X-RAY DIFFRACTION           Protein   \n 5        102M       OXYGEN TRANSPORT     X-RAY DIFFRACTION           Protein   \n 7        103L  HYDROLASE(O-GLYCOSYL)     X-RAY DIFFRACTION           Protein   \n 8        103M       OXYGEN TRANSPORT     X-RAY DIFFRACTION           Protein   \n \n    residueCount  resolution  structureMolecularWeight crystallizationMethod  \\\n 2           154        2.07                  18112.80                   NaN   \n 4           165        1.74                  18926.61                   NaN   \n 5           154        1.84                  18010.64                   NaN   \n 7           167        1.90                  19092.72                   NaN   \n 8           154        2.07                  18093.78                   NaN   \n \n    crystallizationTempK  densityMatthews  densityPercentSol  \\\n 2                   NaN             3.09              60.20   \n 4                   NaN             2.75              55.28   \n 5                   NaN             3.09              60.20   \n 7                   NaN             2.70              54.46   \n 8                   NaN             3.09              60.30   \n \n                                          pdbxDetails  phValue  publicationYear  \n 2  3.0 M AMMONIUM SULFATE, 20 MM TRIS, 1MM EDTA, ...      9.0           1999.0  \n 4                                                NaN      NaN           1993.0  \n 5  3.0 M AMMONIUM SULFATE, 20 MM TRIS, 1MM EDTA, ...      9.0           1999.0  \n 7                                                NaN      NaN           1993.0  \n 8  3.0 M AMMONIUM SULFATE, 20 MM TRIS, 1MM EDTA, ...      9.0           1999.0  ,\n    structureId chainId                                           sequence  \\\n 4         101M       A  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n 7         102L       A  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAAKSE...   \n 8         102M       A  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n 11        103L       A  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNSLDAAK...   \n 12        103M       A  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n \n     residueCount macromoleculeType  \n 4            154           Protein  \n 7            165           Protein  \n 8            154           Protein  \n 11           167           Protein  \n 12           154           Protein  )"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding how many unique proteins and functions there are\nno_dups_proteins['classification'].nunique(), no_dups_proteins['structureId'].nunique()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(4468, 127387)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another package for dataprocessing\nimport seaborn as sns","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's find the function and see how many proteins are attatched to each\nh = no_dups_proteins['classification'].value_counts();h","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"HYDROLASE                                  20425\nTRANSFERASE                                15393\nOXIDOREDUCTASE                             12216\nLYASE                                       4256\nIMMUNE SYSTEM                               3997\n                                           ...  \nrim-binding protein                            1\nCOMPLEX (ISOMERASE/PROTEIN KINASE)             1\nTRANSPORT PROTEIN/CHAPERONE                    1\nTRANSCRIPTION/PROTEIN BINDING/INHIBITOR        1\nSIGNAL                                         1\nName: classification, Length: 4468, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotin the distribution of the classification of stuff. There seems to be a lot of things that are classificed only a few times... \n#That won't help out the model. So I'll kick them out....\nsns.distplot(h, hist= False)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f49d1854210>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tried to merge them just like that... Didn't work because 2 columns were overlapping and it throw a fit... So I'll kick out all irrelevent info\nnew_no_dups = no_dups_proteins[['structureId', \"classification\"]]\nnew_seq = seq_proteins[['structureId', 'sequence']]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_no_dups.head(), new_seq.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"(  structureId         classification\n 2        101M       OXYGEN TRANSPORT\n 4        102L  HYDROLASE(O-GLYCOSYL)\n 5        102M       OXYGEN TRANSPORT\n 7        103L  HYDROLASE(O-GLYCOSYL)\n 8        103M       OXYGEN TRANSPORT,\n    structureId                                           sequence\n 4         101M  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...\n 7         102L  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAAKSE...\n 8         102M  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...\n 11        103L  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNSLDAAK...\n 12        103M  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I'll merge now\ngonna_be_cut_down_one = new_no_dups.set_index('structureId').join(new_seq.set_index('structureId'))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It works!!!!\ngonna_be_cut_down_one.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"                    classification  \\\nstructureId                          \n101M              OXYGEN TRANSPORT   \n102L         HYDROLASE(O-GLYCOSYL)   \n102M              OXYGEN TRANSPORT   \n103L         HYDROLASE(O-GLYCOSYL)   \n103M              OXYGEN TRANSPORT   \n\n                                                      sequence  \nstructureId                                                     \n101M         MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...  \n102L         MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAAKSE...  \n102M         MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...  \n103L         MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNSLDAAK...  \n103M         MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>sequence</th>\n    </tr>\n    <tr>\n      <th>structureId</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>101M</th>\n      <td>OXYGEN TRANSPORT</td>\n      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n    </tr>\n    <tr>\n      <th>102L</th>\n      <td>HYDROLASE(O-GLYCOSYL)</td>\n      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAAKSE...</td>\n    </tr>\n    <tr>\n      <th>102M</th>\n      <td>OXYGEN TRANSPORT</td>\n      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n    </tr>\n    <tr>\n      <th>103L</th>\n      <td>HYDROLASE(O-GLYCOSYL)</td>\n      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNSLDAAK...</td>\n    </tr>\n    <tr>\n      <th>103M</th>\n      <td>OXYGEN TRANSPORT</td>\n      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see how the function of how high the bar I set versus the amount that make it in (aka have that many proteins)\nfor x in range(50):\n    print(5000-(x * 100),len(h[h > (5000-(x * 100))]))","execution_count":14,"outputs":[{"output_type":"stream","text":"5000 3\n4900 3\n4800 3\n4700 3\n4600 3\n4500 3\n4400 3\n4300 3\n4200 4\n4100 4\n4000 4\n3900 5\n3800 5\n3700 5\n3600 5\n3500 5\n3400 6\n3300 6\n3200 6\n3100 7\n3000 7\n2900 7\n2800 8\n2700 8\n2600 9\n2500 10\n2400 10\n2300 10\n2200 11\n2100 11\n2000 12\n1900 12\n1800 13\n1700 14\n1600 16\n1500 16\n1400 17\n1300 19\n1200 23\n1100 25\n1000 25\n900 25\n800 27\n700 29\n600 30\n500 32\n400 39\n300 43\n200 59\n100 80\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The ones that stay are over 1300\nstay = np.asarray(h[(h > 1300)].index)\n\nnew = gonna_be_cut_down_one[gonna_be_cut_down_one.classification.isin(stay)]\n\nnew.describe()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"       classification                                           sequence\ncount          225223                                             225221\nunique             19                                              58603\ntop         HYDROLASE  KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRN...\nfreq            46336                                                596","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>225223</td>\n      <td>225221</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>19</td>\n      <td>58603</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>HYDROLASE</td>\n      <td>KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRN...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>46336</td>\n      <td>596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are nulls here. Gotta shove them out\nnew = new.dropna()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#They both match now\nnew.describe()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"       classification                                           sequence\ncount          225221                                             225221\nunique             19                                              58603\ntop         HYDROLASE  KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRN...\nfreq            46336                                                596","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>225221</td>\n      <td>225221</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>19</td>\n      <td>58603</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>HYDROLASE</td>\n      <td>KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRN...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>46336</td>\n      <td>596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"new.head()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"                                classification  \\\nstructureId                                      \n10GS         TRANSFERASE/TRANSFERASE INHIBITOR   \n10GS         TRANSFERASE/TRANSFERASE INHIBITOR   \n117E                                 HYDROLASE   \n117E                                 HYDROLASE   \n11AS                                    LIGASE   \n\n                                                      sequence  \nstructureId                                                     \n10GS         PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...  \n10GS         PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...  \n117E         TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...  \n117E         TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...  \n11AS         MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>sequence</th>\n    </tr>\n    <tr>\n      <th>structureId</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10GS</th>\n      <td>TRANSFERASE/TRANSFERASE INHIBITOR</td>\n      <td>PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...</td>\n    </tr>\n    <tr>\n      <th>10GS</th>\n      <td>TRANSFERASE/TRANSFERASE INHIBITOR</td>\n      <td>PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...</td>\n    </tr>\n    <tr>\n      <th>117E</th>\n      <td>HYDROLASE</td>\n      <td>TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...</td>\n    </tr>\n    <tr>\n      <th>117E</th>\n      <td>HYDROLASE</td>\n      <td>TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...</td>\n    </tr>\n    <tr>\n      <th>11AS</th>\n      <td>LIGASE</td>\n      <td>MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shoot. Looks like the duplicates got in... I'll remove it now\nnew = new.drop_duplicates()\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Yay it works\nnew.head()","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"                                classification  \\\nstructureId                                      \n10GS         TRANSFERASE/TRANSFERASE INHIBITOR   \n117E                                 HYDROLASE   \n11AS                                    LIGASE   \n11BA                                 HYDROLASE   \n11GS                               TRANSFERASE   \n\n                                                      sequence  \nstructureId                                                     \n10GS         PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...  \n117E         TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...  \n11AS         MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...  \n11BA         KESAAAKFERQHMDSGNSPSSSSNYCNLMMCCRKMTQGKCKPVNTF...  \n11GS         MPPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKA...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>sequence</th>\n    </tr>\n    <tr>\n      <th>structureId</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10GS</th>\n      <td>TRANSFERASE/TRANSFERASE INHIBITOR</td>\n      <td>PPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKAS...</td>\n    </tr>\n    <tr>\n      <th>117E</th>\n      <td>HYDROLASE</td>\n      <td>TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...</td>\n    </tr>\n    <tr>\n      <th>11AS</th>\n      <td>LIGASE</td>\n      <td>MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...</td>\n    </tr>\n    <tr>\n      <th>11BA</th>\n      <td>HYDROLASE</td>\n      <td>KESAAAKFERQHMDSGNSPSSSSNYCNLMMCCRKMTQGKCKPVNTF...</td>\n    </tr>\n    <tr>\n      <th>11GS</th>\n      <td>TRANSFERASE</td>\n      <td>MPPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import packages for the real business (ML)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting it to a train test... To make sure I know the true accuracy of it\nfrom sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(new, train_size = 0.9)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizers... AKA the thing that will break up my sequences into what I'll feed into the LSTM\ntokenize = lambda x: list(str(x))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test it out - It works\n\ntokenize(['hihihi'])","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"['[', \"'\", 'h', 'i', 'h', 'i', 'h', 'i', \"'\", ']']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This will come in handy later on... Basically labels stuff to then execute some functions on it\nTEXT = Field(tokenize=tokenize,batch_first=True,include_lengths=True)\nLABEL = Field(sequential=False, use_vocab=True)","execution_count":25,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since TabularDataset won't take in our Pandas Dataframe (instead takes in CSVs) We're just gonna convert it to CSVs\ntrain.to_csv(\"Protein_train.csv\")\nvalid.to_csv(\"Protein_test.csv\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the fields for the next part...\nfields = [('structureId', None),('classification', LABEL), ('sequence', TEXT)]","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can create our dataset of train and valids, with the fields attatched to them... Then shove it into the iterator\ntrain_thing, valid_thing = TabularDataset.splits(path=\"./\", train='Protein_train.csv', validation=\"Protein_test.csv\", format='CSV', fields=fields, skip_header=True)","execution_count":28,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It works!!!!\ntrain_thing[0], train_thing[0].__dict__.keys(), train_thing[1].sequence","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"(<torchtext.data.example.Example at 0x7f49c45c56d0>,\n dict_keys(['classification', 'sequence']),\n ['A',\n  'D',\n  'P',\n  'G',\n  'G',\n  'S',\n  'H',\n  'H',\n  'H',\n  'H',\n  'H',\n  'S',\n  'R',\n  'K',\n  'T',\n  'Y',\n  'T',\n  'L',\n  'T',\n  'D',\n  'Y',\n  'L',\n  'K',\n  'N',\n  'T',\n  'Y',\n  'R',\n  'L',\n  'K',\n  'L',\n  'Y',\n  'S',\n  'L',\n  'R',\n  'W',\n  'I',\n  'S',\n  'D',\n  'H',\n  'E',\n  'Y',\n  'L',\n  'Y',\n  'K',\n  'Q',\n  'E',\n  'N',\n  'N',\n  'I',\n  'L',\n  'V',\n  'F',\n  'N',\n  'A',\n  'E',\n  'Y',\n  'G',\n  'N',\n  'S',\n  'S',\n  'V',\n  'F',\n  'L',\n  'E',\n  'N',\n  'S',\n  'T',\n  'F',\n  'D',\n  'E',\n  'F',\n  'G',\n  'H',\n  'S',\n  'I',\n  'N',\n  'D',\n  'Y',\n  'S',\n  'I',\n  'S',\n  'P',\n  'D',\n  'G',\n  'Q',\n  'F',\n  'I',\n  'L',\n  'L',\n  'E',\n  'Y',\n  'N',\n  'Y',\n  'V',\n  'K',\n  'Q',\n  'W',\n  'R',\n  'H',\n  'S',\n  'Y',\n  'T',\n  'A',\n  'S',\n  'Y',\n  'D',\n  'I',\n  'Y',\n  'D',\n  'L',\n  'N',\n  'K',\n  'R',\n  'Q',\n  'L',\n  'I',\n  'T',\n  'E',\n  'E',\n  'R',\n  'I',\n  'P',\n  'N',\n  'N',\n  'T',\n  'Q',\n  'W',\n  'V',\n  'T',\n  'W',\n  'S',\n  'P',\n  'V',\n  'G',\n  'H',\n  'K',\n  'L',\n  'A',\n  'Y',\n  'V',\n  'W',\n  'N',\n  'N',\n  'D',\n  'I',\n  'Y',\n  'V',\n  'K',\n  'I',\n  'E',\n  'P',\n  'N',\n  'L',\n  'P',\n  'S',\n  'Y',\n  'R',\n  'I',\n  'T',\n  'W',\n  'T',\n  'G',\n  'K',\n  'E',\n  'D',\n  'I',\n  'I',\n  'Y',\n  'N',\n  'G',\n  'I',\n  'T',\n  'D',\n  'W',\n  'V',\n  'Y',\n  'E',\n  'E',\n  'E',\n  'V',\n  'F',\n  'S',\n  'A',\n  'Y',\n  'S',\n  'A',\n  'L',\n  'W',\n  'W',\n  'S',\n  'P',\n  'N',\n  'G',\n  'T',\n  'F',\n  'L',\n  'A',\n  'Y',\n  'A',\n  'Q',\n  'F',\n  'N',\n  'D',\n  'T',\n  'E',\n  'V',\n  'P',\n  'L',\n  'I',\n  'E',\n  'Y',\n  'S',\n  'F',\n  'Y',\n  'S',\n  'D',\n  'E',\n  'S',\n  'L',\n  'Q',\n  'Y',\n  'P',\n  'K',\n  'T',\n  'V',\n  'R',\n  'V',\n  'P',\n  'Y',\n  'P',\n  'K',\n  'A',\n  'G',\n  'A',\n  'V',\n  'N',\n  'P',\n  'T',\n  'V',\n  'K',\n  'F',\n  'F',\n  'V',\n  'V',\n  'N',\n  'T',\n  'D',\n  'S',\n  'L',\n  'S',\n  'S',\n  'V',\n  'T',\n  'N',\n  'A',\n  'T',\n  'S',\n  'I',\n  'Q',\n  'I',\n  'T',\n  'A',\n  'P',\n  'A',\n  'S',\n  'M',\n  'L',\n  'I',\n  'G',\n  'D',\n  'H',\n  'Y',\n  'L',\n  'C',\n  'D',\n  'V',\n  'T',\n  'W',\n  'A',\n  'T',\n  'Q',\n  'E',\n  'R',\n  'I',\n  'S',\n  'L',\n  'Q',\n  'W',\n  'L',\n  'R',\n  'R',\n  'I',\n  'Q',\n  'N',\n  'Y',\n  'S',\n  'V',\n  'M',\n  'D',\n  'I',\n  'C',\n  'D',\n  'Y',\n  'D',\n  'E',\n  'S',\n  'S',\n  'G',\n  'R',\n  'W',\n  'N',\n  'C',\n  'L',\n  'V',\n  'A',\n  'R',\n  'Q',\n  'H',\n  'I',\n  'E',\n  'M',\n  'S',\n  'T',\n  'T',\n  'G',\n  'W',\n  'V',\n  'G',\n  'R',\n  'F',\n  'R',\n  'P',\n  'S',\n  'E',\n  'P',\n  'H',\n  'F',\n  'T',\n  'L',\n  'D',\n  'G',\n  'N',\n  'S',\n  'F',\n  'Y',\n  'K',\n  'I',\n  'I',\n  'S',\n  'N',\n  'E',\n  'E',\n  'G',\n  'Y',\n  'R',\n  'H',\n  'I',\n  'C',\n  'Y',\n  'F',\n  'Q',\n  'I',\n  'D',\n  'K',\n  'K',\n  'D',\n  'C',\n  'T',\n  'F',\n  'I',\n  'T',\n  'K',\n  'G',\n  'T',\n  'W',\n  'E',\n  'V',\n  'I',\n  'G',\n  'I',\n  'E',\n  'A',\n  'L',\n  'T',\n  'S',\n  'D',\n  'Y',\n  'L',\n  'Y',\n  'Y',\n  'I',\n  'S',\n  'N',\n  'E',\n  'Y',\n  'K',\n  'G',\n  'M',\n  'P',\n  'G',\n  'G',\n  'R',\n  'N',\n  'L',\n  'Y',\n  'K',\n  'I',\n  'Q',\n  'L',\n  'S',\n  'D',\n  'Y',\n  'T',\n  'K',\n  'V',\n  'T',\n  'C',\n  'L',\n  'S',\n  'C',\n  'E',\n  'L',\n  'N',\n  'P',\n  'E',\n  'R',\n  'C',\n  'Q',\n  'Y',\n  'Y',\n  'S',\n  'V',\n  'S',\n  'F',\n  'S',\n  'K',\n  'E',\n  'A',\n  'K',\n  'Y',\n  'Y',\n  'Q',\n  'L',\n  'R',\n  'C',\n  'S',\n  'G',\n  'P',\n  'G',\n  'L',\n  'P',\n  'L',\n  'Y',\n  'T',\n  'L',\n  'H',\n  'S',\n  'S',\n  'V',\n  'N',\n  'D',\n  'K',\n  'G',\n  'L',\n  'R',\n  'V',\n  'L',\n  'E',\n  'D',\n  'N',\n  'S',\n  'A',\n  'L',\n  'D',\n  'K',\n  'M',\n  'L',\n  'Q',\n  'N',\n  'V',\n  'Q',\n  'M',\n  'P',\n  'S',\n  'K',\n  'K',\n  'L',\n  'D',\n  'F',\n  'I',\n  'I',\n  'L',\n  'N',\n  'E',\n  'T',\n  'K',\n  'F',\n  'W',\n  'Y',\n  'Q',\n  'M',\n  'I',\n  'L',\n  'P',\n  'P',\n  'H',\n  'F',\n  'D',\n  'K',\n  'S',\n  'K',\n  'K',\n  'Y',\n  'P',\n  'L',\n  'L',\n  'L',\n  'D',\n  'V',\n  'Y',\n  'A',\n  'G',\n  'P',\n  'C',\n  'S',\n  'Q',\n  'K',\n  'A',\n  'D',\n  'T',\n  'V',\n  'F',\n  'R',\n  'L',\n  'N',\n  'W',\n  'A',\n  'T',\n  'Y',\n  'L',\n  'A',\n  'S',\n  'T',\n  'E',\n  'N',\n  'I',\n  'I',\n  'V',\n  'A',\n  'S',\n  'F',\n  'D',\n  'G',\n  'R',\n  'G',\n  'S',\n  'G',\n  'Y',\n  'Q',\n  'G',\n  'D',\n  'K',\n  'I',\n  'M',\n  'H',\n  'A',\n  'I',\n  'N',\n  'R',\n  'R',\n  'L',\n  'G',\n  'T',\n  'F',\n  'E',\n  'V',\n  'E',\n  'D',\n  'Q',\n  'I',\n  'E',\n  'A',\n  'A',\n  'R',\n  'Q',\n  'F',\n  'S',\n  'K',\n  'M',\n  'G',\n  'F',\n  'V',\n  'D',\n  'N',\n  'K',\n  'R',\n  'I',\n  'A',\n  'I',\n  'W',\n  'G',\n  'W',\n  'S',\n  'Y',\n  'G',\n  'G',\n  'Y',\n  'V',\n  'T',\n  'S',\n  'M',\n  'V',\n  'L',\n  'G',\n  'S',\n  'G',\n  'S',\n  'G',\n  'V',\n  'F',\n  'K',\n  'C',\n  'G',\n  'I',\n  'A',\n  'V',\n  'A',\n  'P',\n  'V',\n  'S',\n  'R',\n  'W',\n  'E',\n  'Y',\n  'Y',\n  'D',\n  'S',\n  'V',\n  'Y',\n  'T',\n  'E',\n  'R',\n  'Y',\n  'M',\n  'G',\n  'L',\n  'P',\n  'T',\n  'P',\n  'E',\n  'D',\n  'N',\n  'L',\n  'D',\n  'H',\n  'Y',\n  'R',\n  'N',\n  'S',\n  'T',\n  'V',\n  'M',\n  'S',\n  'R',\n  'A',\n  'E',\n  'N',\n  'F',\n  'K',\n  'Q',\n  'V',\n  'E',\n  'Y',\n  'L',\n  'L',\n  'I',\n  'H',\n  'G',\n  'T',\n  'A',\n  'D',\n  'D',\n  'N',\n  'V',\n  'H',\n  'F',\n  'Q',\n  'Q',\n  'S',\n  'A',\n  'Q',\n  'I',\n  'S',\n  'K',\n  'A',\n  'L',\n  'V',\n  'D',\n  'V',\n  'G',\n  'V',\n  'D',\n  'F',\n  'Q',\n  'A',\n  'M',\n  'W',\n  'Y',\n  'T',\n  'D',\n  'E',\n  'D',\n  'H',\n  'G',\n  'I',\n  'A',\n  'S',\n  'S',\n  'T',\n  'A',\n  'H',\n  'Q',\n  'H',\n  'I',\n  'Y',\n  'T',\n  'H',\n  'M',\n  'S',\n  'H',\n  'F',\n  'I',\n  'K',\n  'Q',\n  'C',\n  'F',\n  'S',\n  'L',\n  'P'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda'\nbs = 64","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Puts similar things together and then pads them... Plus preps them for iteration so we can shove it into the model with little friction\ntrain_iter = BucketIterator(train_thing, batch_size=bs, sort_key=lambda x: len(x.sequence),\n                            device=device, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid_thing, batch_size=bs, sort_key=lambda x: len(x.sequence),\n                            device=device, sort=True, sort_within_batch=True)","execution_count":31,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make the vocab for the fields... AKA the embedding layers\nTEXT.build_vocab(train_thing)\nLABEL.build_vocab(train_thing)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We're creating the LSTMmodel!!!\nclass LSTMmodel(nn.Module):\n    #On initialization we're gonna make our LSTM + the linear layers\n    def __init__(self, vocab_size, embedding_size, hidden_dim, layers, dropout):\n        super().__init__()\n        \n        #The embedding Layer\n        self.Embedding = nn.Embedding(vocab_size, embedding_size)\n        \n        #The Jewel: LSTM!!!\n        self.LSTM = nn.LSTM(embedding_size, \n                            hidden_dim, \n                            num_layers=layers,\n                            dropout=dropout,\n                            batch_first=True,\n                            bidirectional=True)\n        \n        #Some extra linear layers just for the hell of it\n        self.layers = nn.Sequential(\n            nn.Linear(hidden_dim * 2, 50),\n            nn.BatchNorm1d(50),\n            nn.Sigmoid(),\n            nn.Linear(50, 20),\n            nn.BatchNorm1d(20),\n            nn.Sigmoid())\n        \n    def forward(self, text, text_len):\n        #Pass our text into the the embeddng layer ()\n        embed = self.Embedding(text)\n        #Then we pack pad it... Essentially an optimzied way to pad our stuff without doing extra computations\n        packed_embed = nn.utils.rnn.pack_padded_sequence(embed, text_len,batch_first=True)\n        \n        #Run it through the LSTM. The most valuable thing is h_n, which is the hidden state of the sequence\n        outfeat, (h_n, c_n) = self.LSTM(packed_embed)\n        \n        #re-format it all\n        proto_out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)\n        \n        #Put it through our linear layers\n        \n        outputs = self.layers(proto_out)\n        \n        return outputs\n        ","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the model\n\nmodel = LSTMmodel(len(TEXT.vocab), 100, 32, 4, 0.2)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check it out\nprint(model)","execution_count":35,"outputs":[{"output_type":"stream","text":"LSTMmodel(\n  (Embedding): Embedding(27, 100)\n  (LSTM): LSTM(100, 32, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=50, bias=True)\n    (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Sigmoid()\n    (3): Linear(in_features=50, out_features=20, bias=True)\n    (4): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): Sigmoid()\n  )\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at how many parameters\ntotal = 0\nfor param in model.parameters():\n    if param.requires_grad:\n        total += param.numel()\n\nprint(total)","execution_count":36,"outputs":[{"output_type":"stream","text":"116678\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr = 1e-3)\n\n#Define the loss function\nloss_func = nn.BCELoss()\n\n#Define the metrics function\ndef accuracy(preds, target):\n    correct = (preds == target).float()\n    accuracy = correct.sum() / len(correct)\n    return accuracy","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shove stuff onto GPU\nmodel = model.to(device)\nloss_func = loss_func.to(device)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the trianing function\ndef train(model, iterator, optimizer, loss_func):\n    #Initializing them\n    epoch_loss = 0\n    epoch_accuracy = 0\n    \n    #Gets the model in training mode haha\n    model.train()\n    \n    for batch in iterator:\n        #Set the gradietns to 0\n        optimizer.zero_grad()\n        \n        #Get the text and number of words to pass into model\n        text, text_len = batch.sequence\n        #Because it throws a fit otherwise\n        text_len = text_len.cpu()\n        \n        #Shove it into model\n        preds = model(text, text_len).squeeze()\n        \n        #Turn the target into one hot encoding\n        onehot = F.one_hot(batch.classification,num_classes=20)\n        \n        #Find the loss\n        loss = loss_func(preds, onehot.float())\n        \n        #Find the accuracy\n        acc = accuracy(torch.argmax(preds, dim = 1).float(), batch.classification.float())\n        \n        #Backprop the loss and find the gradients\n        loss.backward()\n        #Then update all the weights\n        optimizer.step()\n        \n        #Add in the loss and the accuracy\n        epoch_loss += loss.item()\n        epoch_accuracy += acc.item()\n    \n    #Return the loss and the accuracy\n    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n        ","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the test function\n\ndef test(model, iterator, loss_func):\n    #Initializing them\n    epoch_loss = 0\n    epoch_accuracy = 0\n    \n    #There's an evaluation mode too\n    model.eval()\n    \n    #So it doesn't add on to the memory to compute the gradients\n    with torch.no_grad():\n        for batch in iterator:\n\n            #Get the text and number of words to pass into model\n            text, text_len = batch.sequence\n            #Because it throws a fit otherwise\n            text_len = text_len.cpu()\n\n            #Shove it into model\n            preds = model(text, text_len).squeeze()\n\n            #Turn the target into one hot encoding\n            onehot = F.one_hot(batch.classification,num_classes=20)\n\n            #Find the loss\n            loss = loss_func(preds, onehot.float())\n\n            #Find the accuracy\n            acc = accuracy(torch.argmax(preds, dim = 1).float(), batch.classification.float())\n\n            #Add in the loss and the accuracy\n            epoch_loss += loss.item()\n            epoch_accuracy += acc.item()\n\n        #Return the loss and the accuracy\n        return epoch_loss / len(iterator), epoch_accuracy / len(iterator)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final Step! Cobbling everything together\nstart_time = time.time()\n\nfor epoch in range(15):\n    epoch_start = time.time()\n    #Train it\n    train_loss, train_acc = train(model, train_iter, optimizer, loss_func)\n    \n    #Validate it\n    valid_loss, valid_acc = test(model, valid_iter, loss_func)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print('Total Time for Epoch:', time.time() - epoch_start)\n\nprint(\"Total Time of all of this:\", time.time() - start_time)","execution_count":42,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"\tTrain Loss: 0.577 | Train Acc: 16.97%\n\t Val. Loss: 0.432 |  Val. Acc: 21.26%\nTotal Time for Epoch: 109.51424145698547\n\tTrain Loss: 0.350 | Train Acc: 25.10%\n\t Val. Loss: 0.292 |  Val. Acc: 26.88%\nTotal Time for Epoch: 108.95476603507996\n\tTrain Loss: 0.258 | Train Acc: 28.89%\n\t Val. Loss: 0.230 |  Val. Acc: 28.26%\nTotal Time for Epoch: 109.53878402709961\n\tTrain Loss: 0.212 | Train Acc: 30.72%\n\t Val. Loss: 0.199 |  Val. Acc: 27.57%\nTotal Time for Epoch: 109.03388047218323\n\tTrain Loss: 0.188 | Train Acc: 32.07%\n\t Val. Loss: 0.188 |  Val. Acc: 25.90%\nTotal Time for Epoch: 109.20695209503174\n\tTrain Loss: 0.175 | Train Acc: 33.31%\n\t Val. Loss: 0.174 |  Val. Acc: 29.75%\nTotal Time for Epoch: 108.65601754188538\n\tTrain Loss: 0.167 | Train Acc: 34.30%\n\t Val. Loss: 0.166 |  Val. Acc: 31.04%\nTotal Time for Epoch: 109.32505393028259\n\tTrain Loss: 0.161 | Train Acc: 36.18%\n\t Val. Loss: 0.163 |  Val. Acc: 32.98%\nTotal Time for Epoch: 109.5779013633728\n\tTrain Loss: 0.156 | Train Acc: 37.56%\n\t Val. Loss: 0.164 |  Val. Acc: 28.79%\nTotal Time for Epoch: 108.89653420448303\n\tTrain Loss: 0.153 | Train Acc: 38.48%\n\t Val. Loss: 0.157 |  Val. Acc: 33.39%\nTotal Time for Epoch: 109.54229235649109\n\tTrain Loss: 0.150 | Train Acc: 40.00%\n\t Val. Loss: 0.156 |  Val. Acc: 35.01%\nTotal Time for Epoch: 109.09142708778381\n\tTrain Loss: 0.147 | Train Acc: 41.21%\n\t Val. Loss: 0.157 |  Val. Acc: 33.23%\nTotal Time for Epoch: 108.26961970329285\n\tTrain Loss: 0.147 | Train Acc: 41.22%\n\t Val. Loss: 0.151 |  Val. Acc: 36.87%\nTotal Time for Epoch: 109.9564516544342\n\tTrain Loss: 0.143 | Train Acc: 42.98%\n\t Val. Loss: 0.152 |  Val. Acc: 36.59%\nTotal Time for Epoch: 108.66684913635254\n\tTrain Loss: 0.141 | Train Acc: 44.07%\n\t Val. Loss: 0.148 |  Val. Acc: 38.11%\nTotal Time for Epoch: 109.25854206085205\nTotal Time of all of this: 1637.4914991855621\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Continue training but at lower LR rate\noptimizer = optim.Adam(model.parameters(), lr = 0.0005)\n\nstart_time = time.time()\n\nfor epoch in range(20):\n    epoch_start = time.time()\n    #Train it\n    train_loss, train_acc = train(model, train_iter, optimizer, loss_func)\n    \n    #Validate it\n    valid_loss, valid_acc = test(model, valid_iter, loss_func)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print('Total Time for Epoch:', time.time() - epoch_start)\n\nprint(\"Total Time of all of this:\", time.time() - start_time)","execution_count":43,"outputs":[{"output_type":"stream","text":"\tTrain Loss: 0.139 | Train Acc: 44.76%\n\t Val. Loss: 0.144 |  Val. Acc: 40.97%\nTotal Time for Epoch: 110.25700545310974\n\tTrain Loss: 0.137 | Train Acc: 45.74%\n\t Val. Loss: 0.143 |  Val. Acc: 41.07%\nTotal Time for Epoch: 109.00415706634521\n\tTrain Loss: 0.136 | Train Acc: 45.97%\n\t Val. Loss: 0.143 |  Val. Acc: 40.90%\nTotal Time for Epoch: 109.22637820243835\n\tTrain Loss: 0.134 | Train Acc: 46.92%\n\t Val. Loss: 0.142 |  Val. Acc: 41.84%\nTotal Time for Epoch: 109.42888569831848\n\tTrain Loss: 0.133 | Train Acc: 47.54%\n\t Val. Loss: 0.142 |  Val. Acc: 42.09%\nTotal Time for Epoch: 108.80709290504456\n\tTrain Loss: 0.132 | Train Acc: 47.86%\n\t Val. Loss: 0.143 |  Val. Acc: 41.68%\nTotal Time for Epoch: 110.74929809570312\n\tTrain Loss: 0.131 | Train Acc: 48.51%\n\t Val. Loss: 0.142 |  Val. Acc: 42.04%\nTotal Time for Epoch: 109.62591814994812\n\tTrain Loss: 0.130 | Train Acc: 49.22%\n\t Val. Loss: 0.142 |  Val. Acc: 42.26%\nTotal Time for Epoch: 109.26461672782898\n\tTrain Loss: 0.128 | Train Acc: 49.50%\n\t Val. Loss: 0.144 |  Val. Acc: 41.39%\nTotal Time for Epoch: 109.18343377113342\n\tTrain Loss: 0.127 | Train Acc: 50.04%\n\t Val. Loss: 0.143 |  Val. Acc: 41.32%\nTotal Time for Epoch: 109.66569256782532\n\tTrain Loss: 0.127 | Train Acc: 50.42%\n\t Val. Loss: 0.142 |  Val. Acc: 42.25%\nTotal Time for Epoch: 108.58159637451172\n\tTrain Loss: 0.125 | Train Acc: 51.07%\n\t Val. Loss: 0.142 |  Val. Acc: 41.83%\nTotal Time for Epoch: 108.80839395523071\n\tTrain Loss: 0.124 | Train Acc: 51.49%\n\t Val. Loss: 0.143 |  Val. Acc: 41.96%\nTotal Time for Epoch: 109.50253963470459\n\tTrain Loss: 0.123 | Train Acc: 51.93%\n\t Val. Loss: 0.144 |  Val. Acc: 42.32%\nTotal Time for Epoch: 108.63425636291504\n\tTrain Loss: 0.123 | Train Acc: 52.23%\n\t Val. Loss: 0.145 |  Val. Acc: 41.43%\nTotal Time for Epoch: 108.98158955574036\n\tTrain Loss: 0.122 | Train Acc: 52.61%\n\t Val. Loss: 0.145 |  Val. Acc: 42.08%\nTotal Time for Epoch: 109.44706058502197\n\tTrain Loss: 0.121 | Train Acc: 53.16%\n\t Val. Loss: 0.144 |  Val. Acc: 42.40%\nTotal Time for Epoch: 108.73967981338501\n\tTrain Loss: 0.121 | Train Acc: 53.18%\n\t Val. Loss: 0.145 |  Val. Acc: 42.19%\nTotal Time for Epoch: 108.983473777771\n\tTrain Loss: 0.120 | Train Acc: 53.73%\n\t Val. Loss: 0.147 |  Val. Acc: 42.11%\nTotal Time for Epoch: 108.19844603538513\n\tTrain Loss: 0.119 | Train Acc: 53.97%\n\t Val. Loss: 0.147 |  Val. Acc: 41.64%\nTotal Time for Epoch: 108.66394472122192\nTotal Time of all of this: 2183.756374359131\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Continue training but at lower LR rate\noptimizer = optim.Adam(model.parameters(), lr = 0.0001)\n\nstart_time = time.time()\n\nfor epoch in range(25):\n    epoch_start = time.time()\n    #Train it\n    train_loss, train_acc = train(model, train_iter, optimizer, loss_func)\n    \n    #Validate it\n    valid_loss, valid_acc = test(model, valid_iter, loss_func)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print('Total Time for Epoch:', time.time() - epoch_start)\n\nprint(\"Total Time of all of this:\", time.time() - start_time)","execution_count":44,"outputs":[{"output_type":"stream","text":"\tTrain Loss: 0.121 | Train Acc: 53.10%\n\t Val. Loss: 0.142 |  Val. Acc: 44.28%\nTotal Time for Epoch: 108.52799010276794\n\tTrain Loss: 0.119 | Train Acc: 53.55%\n\t Val. Loss: 0.140 |  Val. Acc: 45.13%\nTotal Time for Epoch: 109.02575874328613\n\tTrain Loss: 0.118 | Train Acc: 53.97%\n\t Val. Loss: 0.142 |  Val. Acc: 44.59%\nTotal Time for Epoch: 108.34387874603271\n\tTrain Loss: 0.118 | Train Acc: 54.28%\n\t Val. Loss: 0.139 |  Val. Acc: 45.39%\nTotal Time for Epoch: 109.82124304771423\n\tTrain Loss: 0.118 | Train Acc: 54.38%\n\t Val. Loss: 0.138 |  Val. Acc: 45.96%\nTotal Time for Epoch: 110.01902914047241\n\tTrain Loss: 0.117 | Train Acc: 54.69%\n\t Val. Loss: 0.138 |  Val. Acc: 46.00%\nTotal Time for Epoch: 109.31956315040588\n\tTrain Loss: 0.117 | Train Acc: 54.67%\n\t Val. Loss: 0.138 |  Val. Acc: 45.88%\nTotal Time for Epoch: 109.51707363128662\n\tTrain Loss: 0.117 | Train Acc: 54.91%\n\t Val. Loss: 0.138 |  Val. Acc: 46.12%\nTotal Time for Epoch: 109.72963881492615\n\tTrain Loss: 0.117 | Train Acc: 55.08%\n\t Val. Loss: 0.137 |  Val. Acc: 46.22%\nTotal Time for Epoch: 109.12595820426941\n\tTrain Loss: 0.116 | Train Acc: 55.18%\n\t Val. Loss: 0.138 |  Val. Acc: 46.30%\nTotal Time for Epoch: 109.99772644042969\n\tTrain Loss: 0.116 | Train Acc: 55.11%\n\t Val. Loss: 0.136 |  Val. Acc: 47.13%\nTotal Time for Epoch: 109.18080544471741\n\tTrain Loss: 0.117 | Train Acc: 54.99%\n\t Val. Loss: 0.135 |  Val. Acc: 47.32%\nTotal Time for Epoch: 109.60013365745544\n\tTrain Loss: 0.116 | Train Acc: 55.11%\n\t Val. Loss: 0.136 |  Val. Acc: 47.02%\nTotal Time for Epoch: 108.57625913619995\n\tTrain Loss: 0.116 | Train Acc: 55.54%\n\t Val. Loss: 0.136 |  Val. Acc: 47.28%\nTotal Time for Epoch: 108.92145609855652\n\tTrain Loss: 0.115 | Train Acc: 55.64%\n\t Val. Loss: 0.136 |  Val. Acc: 47.12%\nTotal Time for Epoch: 109.47416615486145\n\tTrain Loss: 0.115 | Train Acc: 55.55%\n\t Val. Loss: 0.136 |  Val. Acc: 47.03%\nTotal Time for Epoch: 109.28921389579773\n\tTrain Loss: 0.115 | Train Acc: 55.77%\n\t Val. Loss: 0.137 |  Val. Acc: 47.03%\nTotal Time for Epoch: 109.19493198394775\n\tTrain Loss: 0.115 | Train Acc: 55.85%\n\t Val. Loss: 0.136 |  Val. Acc: 47.43%\nTotal Time for Epoch: 109.60695791244507\n\tTrain Loss: 0.114 | Train Acc: 56.13%\n\t Val. Loss: 0.135 |  Val. Acc: 47.33%\nTotal Time for Epoch: 108.12466263771057\n\tTrain Loss: 0.114 | Train Acc: 56.07%\n\t Val. Loss: 0.137 |  Val. Acc: 47.04%\nTotal Time for Epoch: 109.96504378318787\n\tTrain Loss: 0.114 | Train Acc: 56.37%\n\t Val. Loss: 0.136 |  Val. Acc: 47.43%\nTotal Time for Epoch: 109.13546991348267\n\tTrain Loss: 0.114 | Train Acc: 56.10%\n\t Val. Loss: 0.136 |  Val. Acc: 47.69%\nTotal Time for Epoch: 108.73736095428467\n\tTrain Loss: 0.114 | Train Acc: 56.25%\n\t Val. Loss: 0.136 |  Val. Acc: 47.44%\nTotal Time for Epoch: 110.30128693580627\n\tTrain Loss: 0.113 | Train Acc: 56.50%\n\t Val. Loss: 0.136 |  Val. Acc: 47.36%\nTotal Time for Epoch: 109.33449840545654\n\tTrain Loss: 0.113 | Train Acc: 56.57%\n\t Val. Loss: 0.135 |  Val. Acc: 47.62%\nTotal Time for Epoch: 109.84974431991577\nTotal Time of all of this: 2732.722709417343\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Continue training but at lower LR rate\noptimizer = optim.Adam(model.parameters(), lr = 0.0001)\n\nstart_time = time.time()\n\nfor epoch in range(25):\n    epoch_start = time.time()\n    #Train it\n    train_loss, train_acc = train(model, train_iter, optimizer, loss_func)\n    \n    #Validate it\n    valid_loss, valid_acc = test(model, valid_iter, loss_func)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print('Total Time for Epoch:', time.time() - epoch_start)\n\nprint(\"Total Time of all of this:\", time.time() - start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Continue training but at lower LR rate\noptimizer = optim.Adam(model.parameters(), lr = 0.00005)\n\nstart_time = time.time()\n\nfor epoch in range(25):\n    epoch_start = time.time()\n    #Train it\n    train_loss, train_acc = train(model, train_iter, optimizer, loss_func)\n    \n    #Validate it\n    valid_loss, valid_acc = test(model, valid_iter, loss_func)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print('Total Time for Epoch:', time.time() - epoch_start)\n\nprint(\"Total Time of all of this:\", time.time() - start_time)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}